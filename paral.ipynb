{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization\n",
    "\n",
    "- Parallel programming is hard.\n",
    "- The main challenge is multiple tasks accessing the same resource.\n",
    "- With multithreading, order of execution may change in subtle ways.\n",
    "- Fortunately, many computational tasks are [\"embarassingly parallel\"](https://en.wikipedia.org/wiki/Embarrassingly_parallel), and parallelization can provide great speedups at low cost.\n",
    "- Parallelization will always multiply memory usage. Won't help if your processing is *memory bound*.\n",
    "- To parallelize your code, you can use multiple **threads** or multiple **processes**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading\n",
    "\n",
    "Threads are lightweight and share the same memory space.\n",
    "\n",
    "- In Python, because of GIL (global interpreter lock), only one thread can be executed at a time.\n",
    "- Parallelization is achieved by switching between threads when they idle.\n",
    "- Best used with *I/O bound* tasks (CPU load under 100% is good indicator).\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import concurrent.futures\n",
    "from time import time, sleep\n",
    "\n",
    "def cpu_idle(t):\n",
    "    thread = threading.current_thread().name\n",
    "    print(thread, 'says hello')\n",
    "    sleep(t)\n",
    "    print(thread, 'says bye')\n",
    "    \n",
    "def cpu_burn(t):\n",
    "    thread = threading.current_thread().name\n",
    "    print(thread, 'says hello')\n",
    "    t0 = time()\n",
    "    while time() - t0 < t:\n",
    "        x = 1\n",
    "    print(thread, 'says bye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# sequential\n",
    "times = [2] * 3\n",
    "for _ in map(cpu_idle, times):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the problem of writing to a shared resource from multiple threads. In this case, shared resource is the standard output stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# multithreaded, low CPU load\n",
    "times = [2] * 3\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(times)) as pool:\n",
    "    pool.map(cpu_idle, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# multithreaded, high CPU load\n",
    "# if you see no difference, increase length of list\n",
    "times = [2] * 3\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(times)) as pool:\n",
    "    pool.map(cpu_burn, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: download many files\n",
    "\n",
    "This task is network I/O bound, CPU is idling while waiting for the next chunk of data to arrive.\n",
    "\n",
    "[Cartographic boundary files](https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html) - Census Bureau\n",
    "\n",
    "> The cartographic boundary files are simplified representations of selected geographic areas from the Census Bureauâ€™s MAF/TIGER geographic database. These boundary files are specifically designed for small scale thematic mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "from tools import download_file, unzip, ResourceMonitor, tracts_state_00_aa\n",
    "\n",
    "def download_state_tracts(state_code):\n",
    "    url = f'https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_{state_code}_tract_500k.zip'\n",
    "    f = download_file(url, f'data/tracts/{state_code}', overwrite=True, verbose=False)\n",
    "    print(threading.current_thread().name, 'finished', state_code)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = ResourceMonitor()\n",
    "mon.start()\n",
    "\n",
    "for _ in map(download_state_tracts, tracts_state_00_aa):\n",
    "    pass\n",
    "\n",
    "mon.stop()\n",
    "mon.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel\n",
    "With multiple threads CPU can go over 100%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = ResourceMonitor(interval=0.2)\n",
    "mon.start()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as pool:\n",
    "    pool.map(download_state_tracts, tracts_state_00_aa)\n",
    "\n",
    "mon.stop()\n",
    "mon.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing\n",
    "\n",
    "Multiprocessing spawns subprocesses and initial memory state is cloned to each.\n",
    "\n",
    "- Every process then has independent memory space. Less risk of corrupting shared state.\n",
    "- Because initial memory is copied to each process, memory usage is higher.\n",
    "- Best used with *CPU bound* tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "from time import time, sleep\n",
    "\n",
    "def cpu_idle(t):\n",
    "    print(os.getpid(), 'says hello')\n",
    "    sleep(t)\n",
    "    print(os.getpid(), 'says bye')\n",
    "    \n",
    "def cpu_burn(t):\n",
    "    print(os.getpid(), 'says hello')\n",
    "    t0 = time()\n",
    "    while time() - t0 < t:\n",
    "        x = 1\n",
    "    print(os.getpid(), 'says bye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# multiprocessing, low CPU load\n",
    "times = [2] * 3\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=len(times)) as pool:\n",
    "    pool.map(cpu_idle, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# multiprocessing, high CPU load\n",
    "times = [2] * 3\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=len(times)) as pool:\n",
    "    pool.map(cpu_burn, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory under multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "import os\n",
    "import concurrent.futures\n",
    "from time import time, sleep\n",
    "import psutil\n",
    "\n",
    "x = [1] * 20_000_000\n",
    "\n",
    "def use_mem(i):\n",
    "    p = psutil.Process()\n",
    "    # every process gets different delay to avoid simultaneous writes to stdout\n",
    "    sleep(i / 2)\n",
    "    mem = p.memory_info().rss // 2**20\n",
    "    print(i, os.getpid(), 'initial usage:', mem, 'MB')\n",
    "    sleep(3)\n",
    "    mem = p.memory_info().rss // 2**20\n",
    "    print(i, os.getpid(), 'usage:', mem, 'MB')\n",
    "    # make a copy of x\n",
    "    y = list(x)\n",
    "    sleep(1)\n",
    "    mem = p.memory_info().rss // 2**20\n",
    "    print(i, os.getpid(), 'final usage:', mem, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = 2\n",
    "with concurrent.futures.ProcessPoolExecutor(n) as pool:\n",
    "    pool.map(use_mem, range(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: identify census tracts from coordinates\n",
    "\n",
    "For every establishment, we want to identify census tract that it belongs to. This requires to perform \"point in shape\" computation, CPU intensive task, many times. Memory usage is not high, and so we will parallelize the task over multiple processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from tools import download_file, unzip, tracts_state_00_aa, tracts_state_aa_00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip downloaded shapefiles\n",
    "\n",
    "Files are small, so this is very fast to do sequentially. But this task could also be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_tract(state_code):\n",
    "    f = f'data/tracts/{state_code}/cb_2019_{state_code}_tract_500k.zip'\n",
    "    unzip(f, f'data/tracts/{state_code}', overwrite=True, verbose=False)\n",
    "\n",
    "for sc in tracts_state_00_aa:\n",
    "    unzip_tract(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify tracts\n",
    "\n",
    "Perform \"spatial join\" of all establishment coordinats against all tract shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracts_from_coords(state):\n",
    "    state_code = tracts_state_aa_00[state]\n",
    "    df = pd.read_parquet('data/synig.pq', columns=['ABI', 'LONGITUDE', 'LATITUDE'],\n",
    "                         filters=[('YEAR', '==', 2001), ('STATE', '==', state)])\n",
    "    if len(df) == 0:\n",
    "        return\n",
    "    df = gpd.GeoDataFrame(df)\n",
    "    df['LONLAT'] = gpd.points_from_xy(df['LONGITUDE'], df['LATITUDE'])\n",
    "    df = df.set_geometry('LONLAT', crs={'init': 'epsg:4326'})\n",
    "    tracts = gpd.read_file(f'data/tracts/{state_code}/cb_2019_{state_code}_tract_500k.shp')\n",
    "    tracts = tracts[['GEOID', 'geometry']].to_crs({'init': 'epsg:4326'})\n",
    "    df = gpd.sjoin(df, tracts, 'left', 'within')\n",
    "    return df[['ABI', 'GEOID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# sequential execution is rather slow\n",
    "# short list of states for demonstration\n",
    "states = list(tracts_state_aa_00.keys())[:4]\n",
    "\n",
    "df = []\n",
    "for state in states:\n",
    "    print(state, end=' ')\n",
    "    df.append(tracts_from_coords(state))\n",
    "print()\n",
    "\n",
    "df = pd.concat([x for x in df if x is not None], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# parallel execution\n",
    "# short list of states for demonstration\n",
    "states = list(tracts_state_aa_00.keys())[:4]\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(4) as pool:\n",
    "    df = pool.map(tracts_from_coords, states)\n",
    "df = pd.concat([x for x in df if x is not None], ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
