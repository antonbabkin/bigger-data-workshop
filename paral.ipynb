{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization\n",
    "\n",
    "- Parallel programming is hard. \n",
    "- Fortunately, many computational tasks are [\"embarassingly parallel\"](https://en.wikipedia.org/wiki/Embarrassingly_parallel), and parallelization can provide great speedups at low cost.\n",
    "- The main challenge is multiple tasks accessing the same resource.\n",
    "- With multithreading, order of execution may change in subtle ways.\n",
    "- Parallelization will always multiply memory usage. Won't help if your processing is *memory bound*.\n",
    "\n",
    "- To parallelize your code, you can use multiple **threads** or multiple **processes**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading\n",
    "\n",
    "Threads are lightweight and share the same memory space.\n",
    "\n",
    "- In Python, because of GIL (global interpreter lock), only one thread can be executed at a time.\n",
    "- Parallelization is achieved by switching between threads when they idle.\n",
    "- Best used with *I/O bound* tasks (CPU load under 100% is good indicator).\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import concurrent.futures\n",
    "from time import time, sleep\n",
    "\n",
    "def cpu_idle(t):\n",
    "    thread = threading.current_thread().name\n",
    "    print(thread, 'says hello')\n",
    "    sleep(t)\n",
    "    print(thread, 'says bye')\n",
    "    \n",
    "def cpu_burn(t):\n",
    "    thread = threading.current_thread().name\n",
    "    print(thread, 'says hello')\n",
    "    t0 = time()\n",
    "    while time() - t0 < t:\n",
    "        x = 1\n",
    "    print(thread, 'says bye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# sequential\n",
    "times = [2] * 3\n",
    "for t in times:\n",
    "    cpu_idle(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# multithreaded\n",
    "times = [2] * 3\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(times)) as pool:\n",
    "    pool.map(cpu_idle, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# multithreaded CPU-bound\n",
    "# if you see no difference, increase length of list\n",
    "times = [2] * 3\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(times)) as pool:\n",
    "    pool.map(cpu_burn, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: download many files\n",
    "\n",
    "This task is network I/O bound, CPU is idling while waiting for the next chunk of data to arrive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cartographic boundary files](https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html) - Census\n",
    "\n",
    "> The cartographic boundary files are **simplified** representations of selected geographic areas from the Census Bureauâ€™s MAF/TIGER geographic database. These boundary files are specifically designed for small scale thematic mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "from tools import download_file, unzip, ResourceMonitor, tracts_state_00_aa, tracts_state_aa_00\n",
    "\n",
    "def download_state_tracts(state_code):\n",
    "    url = f'https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_{state_code}_tract_500k.zip'\n",
    "    f = download_file(url, f'data/tracts/{state_code}', overwrite=True, verbose=False)\n",
    "    print(threading.current_thread().name, 'finished', state_code)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = ResourceMonitor(interval=0.5)\n",
    "mon.start()\n",
    "\n",
    "file_paths = []\n",
    "for sc in tracts_state_00_aa.keys():\n",
    "    file_paths.append(download_state_tracts(sc))\n",
    "\n",
    "mon.stop()\n",
    "mon.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel\n",
    "With multiple threads CPU can go over 100%.\n",
    "\n",
    "Problem of shared resources. In this case - standard output stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = ResourceMonitor(interval=0.2)\n",
    "mon.start()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as pool:\n",
    "    futures = []\n",
    "    for sc in state_codes:\n",
    "        futures.append(pool.submit(download_state_tracts, sc))\n",
    "concurrent.futures.wait(futures)\n",
    "file_paths = [f.result() for f in futures]\n",
    "\n",
    "mon.stop()\n",
    "mon.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing\n",
    "\n",
    "Multiprocessing spawns subprocesses and initial memory state is cloned to each.\n",
    "\n",
    "- Every process then has independent memory space. Less risk of corrupting shared state.\n",
    "- Because initial memory is copied to each process, memory usage is higher.\n",
    "- Best used with *CPU bound* tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "from time import time, sleep\n",
    "\n",
    "def cpu_idle(t):\n",
    "    print(os.getpid(), 'says hello')\n",
    "    sleep(t)\n",
    "    print(os.getpid(), 'says bye')\n",
    "    \n",
    "def cpu_burn(t):\n",
    "    print(os.getpid(), 'says hello')\n",
    "    t0 = time()\n",
    "    while time() - t0 < t:\n",
    "        x = 1\n",
    "    print(os.getpid(), 'says bye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "times = [2] * 3\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=len(times)) as pool:\n",
    "    pool.map(cpu_idle, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# multithreaded CPU-bound\n",
    "# if you see no difference, increase length of list\n",
    "times = [2] * 3\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=len(times)) as pool:\n",
    "    pool.map(cpu_burn, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory under multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "import os\n",
    "import concurrent.futures\n",
    "from time import time, sleep\n",
    "import psutil\n",
    "\n",
    "x = [1] * 50_000_000\n",
    "\n",
    "def use_mem(i):\n",
    "    p = psutil.Process()\n",
    "    sleep(i / 2)\n",
    "    mem = p.memory_info().rss // 2**20\n",
    "    print(i, os.getpid(), 'initial usage:', mem, 'MB')\n",
    "    sleep(3)\n",
    "    mem = p.memory_info().rss // 2**20\n",
    "    print(i, os.getpid(), 'usage:', mem, 'MB')\n",
    "    # this makes a copy of x\n",
    "    y = list(x)\n",
    "    sleep(1)\n",
    "    mem = p.memory_info().rss // 2**20\n",
    "    print(i, os.getpid(), 'final usage:', mem, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = 2\n",
    "with concurrent.futures.ProcessPoolExecutor(n) as pool:\n",
    "    pool.map(use_mem, range(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: identify census tracts from coordinates\n",
    "\n",
    "This requires to perform \"point in shape\" computation, CPU intensive task, many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fastparquet\n",
    "from tools import download_file, unzip, tracts_state_00_aa, tracts_state_aa_00, ResourceMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_tract(state_code):\n",
    "    f = f'data/tracts/{state_code}/cb_2019_{state_code}_tract_500k.zip'\n",
    "    unzip(f, f'data/tracts/{state_code}', overwrite=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for sc in tracts_state_00_aa:\n",
    "    unzip_tract(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with concurrent.futures.ProcessPoolExecutor() as pool:\n",
    "    pool.map(unzip_tract, tracts_state_00_aa.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracts_from_coords(state):\n",
    "    state_code = tracts_state_aa_00[state]\n",
    "    df = pd.read_parquet('data/synig.pq', columns=['ABI', 'LONGITUDE', 'LATITUDE'],\n",
    "                         filters=[('YEAR', '==', 2001), ('STATE', '==', state)])\n",
    "    if len(df) == 0:\n",
    "        return\n",
    "    df = gpd.GeoDataFrame(df)\n",
    "    df['LONLAT'] = gpd.points_from_xy(df['LONGITUDE'], df['LATITUDE'])\n",
    "    df = df.set_geometry('LONLAT', crs={'init': 'epsg:4326'})\n",
    "    tracts = gpd.read_file(f'data/tracts/{state_code}/cb_2019_{state_code}_tract_500k.shp')\n",
    "    tracts = tracts[['GEOID', 'geometry']].to_crs({'init': 'epsg:4326'})\n",
    "    df = gpd.sjoin(df, tracts, 'left', 'within')\n",
    "    return df[['ABI', 'GEOID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "states = list(tracts_state_aa_00.keys())[:5]\n",
    "\n",
    "mon = ResourceMonitor()\n",
    "mon.start()\n",
    "df = []\n",
    "for state in states:\n",
    "    print(state, end=' ')\n",
    "    df.append(tracts_from_coords(state))\n",
    "print()\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "mon.stop()\n",
    "mon.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "states = list(tracts_state_aa_00.keys())[:5]\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(10) as pool:\n",
    "    df = pool.map(tracts_from_coords, tracts_state_aa_00.keys())\n",
    "df = [x for x in df if x is not None]\n",
    "df = pd.concat(df, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
