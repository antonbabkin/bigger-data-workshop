{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting and split-apply-combine\n",
    "\n",
    "Subsetting: only load columns and rows that you need.\n",
    "\n",
    "Split-apply-combine strategy:\n",
    "- **split** your data in smaller subsets\n",
    "- **apply** necessary transformation to subsets one at a time, storing transformation results\n",
    "- **combine** results from subsets to get final result\n",
    "\n",
    "![split-apply-combine](https://blog.dask.org/images/split-apply-combine.png)  \n",
    "Image source: [Dask blog](https://blog.dask.org/2019/10/08/df-groupby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bigger than memory\n",
    "\n",
    "Loading the whole example dataset in a DataFrame (20M rows, 15 cols) will take about 1 minute and occupy 4GB+ in memory. More memory will be used if you start running computations. Not feasible in Binder environment where memory is limited to 1-2GB, it will crash and restart your kernel. Multiply by 10 for actual InfoGroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this cell in Binder will restart your kernel\n",
    "df = []\n",
    "for year in range(2001, 2021):\n",
    "    df.append(pd.read_csv(f'data/synig/{year}.csv'))\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use a subset for development and testing\n",
    "\n",
    "If data rows are in random order, reading just the first few rows will give you a representative sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for year in range(2001, 2021):\n",
    "    df.append(pd.read_csv(f'data/synig/{year}.csv', nrows=10_000))\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('SECTOR')['EMPLOYEES'].agg(['size', 'sum', 'mean']).astype(int).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not getting all sectors of the economy here. Clearly, row order is not random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a random sample\n",
    "\n",
    "Let's create a random 1% sample. I will only use subset of years to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for year in range(2001, 2006):\n",
    "    d = pd.read_csv(f'data/synig/{year}.csv')\n",
    "    d = d.sample(frac=0.01)\n",
    "    df.append(d)\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a better random sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with this simple approach on our dataset: longitudinal histories are broken. It won't help if we could even load all years of data and sample from that. Solution: draw random sample of unique identifiers and then get full histories for those identifiers. This approach will yield a sample that has the same distribution as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abi = []\n",
    "for year in range(2001, 2021):\n",
    "    abi.append(pd.read_csv(f'data/synig/{year}.csv', usecols=['ABI']))\n",
    "abi = pd.concat(abi)\n",
    "abi = abi.drop_duplicates()\n",
    "abi = abi.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will take about 1.5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for year in range(2001, 2021):\n",
    "    d = pd.read_csv(f'data/synig/{year}.csv')\n",
    "    d = d.merge(abi, 'left', 'ABI', indicator=True)\n",
    "    d = d[d['_merge'] == 'both']\n",
    "    del d['_merge']\n",
    "    df.append(d)\n",
    "    print('finished year', year)\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this lightweight sample to get some insights about the whole, for example, compare sector sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('SECTOR')['EMPLOYEES'].agg(['size', 'sum', 'mean']).astype(int).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## persist intermediate data for later use\n",
    "\n",
    "You can save dataframe as CSV, `parquet` or some other storage format. Or use standard Python `pickle` module. Here I am using `joblib`. `joblib` can save arbitrary Python objects to disk, with special attention to arrays and dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(df, 'data/rand_1pct.pkl')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "df = joblib.load('data/rand_1pct.pkl')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example 1: annual aggregates\n",
    "\n",
    "For each year, compute total number of establishments, total and average employment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 1: load all years into single dataframe\n",
    "\n",
    "If we could fit data in memory, we would simply do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "df = joblib.load('data/rand_1pct.pkl')\n",
    "df.groupby('YEAR')['EMPLOYEES'].agg(['size', 'sum', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually even with 1GB memory limit, we are able to load all years of data, because we only need two columns. Let's do this for  comparison with other methods.\n",
    "\n",
    "*If this hits memory limit, try after restarting kernel or reduce number of years.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for year in range(2001, 2021):\n",
    "    print(year, end=' ')\n",
    "    d = pd.read_csv(f'data/synig/{year}.csv', usecols=['YEAR', 'EMPLOYEES'])\n",
    "    df.append(d)\n",
    "print()\n",
    "df = pd.concat(df)\n",
    "result = df.groupby('YEAR')['EMPLOYEES'].agg(['size', 'sum', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.astype(int).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 2: split by year\n",
    "\n",
    "Now let's use split-apply-combine. We load data year by year, compute aggregates and save them, and then combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(index=['size', 'sum', 'mean'])\n",
    "# split by year\n",
    "for year in range(2001, 2021):\n",
    "    print(year, end=' ')\n",
    "    df = pd.read_csv(f'data/synig/{year}.csv', usecols=['EMPLOYEES'])\n",
    "    # apply transformation\n",
    "    res = df.agg(['size', 'sum', 'mean'])\n",
    "    # combine year into final result\n",
    "    result[year] = res\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 3: split by year and chunking\n",
    "\n",
    "Let's pretend that loading even a single year at a time is not feasible. We can use `pd.read_csv(chunksize=)` to only load a small chunk of the whole CSV. This becomes a nested split-apply-combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(index=['size', 'sum', 'mean'])\n",
    "# split by year\n",
    "for year in range(2001, 2021):\n",
    "    print(year, end=': ')\n",
    "    res = pd.DataFrame(index=['size', 'sum'])\n",
    "    # split by chunk\n",
    "    for i, d in enumerate(pd.read_csv(f'data/synig/{year}.csv', usecols=['EMPLOYEES'], chunksize=100_000)):\n",
    "        print(i, end=' ')\n",
    "        # apply transformation\n",
    "        # note: we don't compute mean here, because combining chunk means is not trivial\n",
    "        res[i] = d.agg(['size', 'sum'])\n",
    "        count_notna += d['EMPLOYEES'].notna().sum()\n",
    "    # combine chunk results\n",
    "    res = res.sum(1) \n",
    "    res['mean'] = res['sum'] / count_notna\n",
    "    # combine year into final result\n",
    "    result[year] = res\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measuring\n",
    "\n",
    "We will use `ResourceMonitor` to compare resource usage between three methods.\n",
    "\n",
    "To make sure that no data is hanging in memory between runs, I wrap each method in a function, so dataframes can be garbage collected once function returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel before running\n",
    "import time\n",
    "import pandas as pd\n",
    "from tools import ResourceMonitor\n",
    "\n",
    "years = range(2001, 2021)\n",
    "\n",
    "def method1_load_all():\n",
    "    print('Start method 1')\n",
    "    t0 = time.time()\n",
    "    df = []\n",
    "    for year in years:\n",
    "        print(year, end=' ')\n",
    "        d = pd.read_csv(f'data/synig/{year}.csv', usecols=['YEAR', 'EMPLOYEES'])\n",
    "        df.append(d)\n",
    "    print()\n",
    "    df = pd.concat(df)\n",
    "    result = df.groupby('YEAR')['EMPLOYEES'].agg(['size', 'sum', 'mean'])\n",
    "    dt = time.time() - t0\n",
    "    print(f'Finish method 1 in {dt:.1f} seconds\\n')\n",
    "    return result.T\n",
    "\n",
    "def method2_split_by_year():\n",
    "    print('Start method 2')\n",
    "    t0 = time.time()\n",
    "    result = pd.DataFrame(index=['size', 'sum', 'mean'])\n",
    "    for year in years:\n",
    "        print(year, end=' ')\n",
    "        df = pd.read_csv(f'data/synig/{year}.csv', usecols=['EMPLOYEES'])\n",
    "        res = df.agg(['size', 'sum', 'mean'])\n",
    "        result[year] = res\n",
    "    print()\n",
    "    dt = time.time() - t0\n",
    "    print(f'Finish method 2 in {dt:.1f} seconds\\n')\n",
    "    return result\n",
    "\n",
    "def method3_split_by_year_chunk():\n",
    "    print('Start method 3')\n",
    "    t0 = time.time()\n",
    "    result = pd.DataFrame(index=['size', 'sum', 'mean'])\n",
    "    for year in years:\n",
    "        print(year, end=': ')\n",
    "        res = pd.DataFrame(index=['size', 'sum'])\n",
    "        count_notna = 0\n",
    "        for i, d in enumerate(pd.read_csv(f'data/synig/{year}.csv', usecols=['EMPLOYEES'], chunksize=100_000)):\n",
    "            print(i, end=' ')\n",
    "            res[i] = d.agg(['size', 'sum'])\n",
    "            count_notna += d['EMPLOYEES'].notna().sum()\n",
    "        res = res.sum(1) \n",
    "        res['mean'] = res['sum'] / count_notna\n",
    "        result[year] = res\n",
    "        print()\n",
    "    dt = time.time() - t0\n",
    "    print(f'Finish method 3 in {dt:.1f} seconds\\n')\n",
    "    return result\n",
    "\n",
    "\n",
    "mon = ResourceMonitor()\n",
    "mon.start()\n",
    "time.sleep(1) # give monitor time to start\n",
    "mon.tag('method 1 V')\n",
    "r1 = method1_load_all()\n",
    "time.sleep(1)\n",
    "mon.tag('method 2 V')\n",
    "r2 = method2_split_by_year()\n",
    "time.sleep(1)\n",
    "mon.tag('method 3 V')\n",
    "r3 = method3_split_by_year_chunk()\n",
    "time.sleep(1)\n",
    "mon.stop()\n",
    "\n",
    "# make sure that all methods yield same results\n",
    "assert (r1 == r2).all().all()\n",
    "assert (r1 == r3).all().all()\n",
    "\n",
    "mon.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-time-complexity trade-offs\n",
    "\n",
    "|                 | Method 1 | Method 2 | Method 3 |\n",
    "|-----------------|:--------:|:--------:|:--------:|\n",
    "| Code complexity |    low   |  medium  |   high   |\n",
    "| Running time    |   fast   |  medium  |   slow   |\n",
    "| Memory usage    |   high   |  medium  |    low   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example 2: size vs age\n",
    "\n",
    "We want to see how establishment size (number of employees) correlates with its age. Age variable is not available in the data, and to construct it for a single establishment, we need to know the entire history of that establishment.\n",
    "\n",
    "If we could load the whole dataset in memory, it would be something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "df = joblib.load('data/rand_1pct.pkl')\n",
    "df = df.sort_values(['ABI', 'YEAR'])\n",
    "fy = df.groupby('ABI')['YEAR'].min()\n",
    "fy.name = 'FIRST_YEAR'\n",
    "df = df.merge(fy, 'left', 'ABI')\n",
    "df['AGE'] = df['YEAR'] - df['FIRST_YEAR']\n",
    "result = df.groupby('AGE')['EMPLOYEES'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this on whole dataset will use over 2GB of memory even if we load only necessary columns. We need to use split-apply-combine.\n",
    "\n",
    "We could, for example, split our data by state. Assuming that establishments do not move between states over their life time, every state subset will have complete histories. We could then iterate over states, construct age in each and compute desired statistics, and then combine state results into final result (again, paying attention to aggregation of means).\n",
    "\n",
    "But our data is organized by year. Iterating over states will require a preliminary step of saving data split by state. This is a feasible strategy (and becomes much easier if we use partitioned parquet for storage), but we can use a different algorithm that will use existing split by year.\n",
    "\n",
    "The key is to recognize that to compute establishment age in any given year, we only need to know its age in the previous year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tools import ResourceMonitor\n",
    "\n",
    "mon = ResourceMonitor()\n",
    "mon.start()\n",
    "\n",
    "years = range(2001, 2021)\n",
    "\n",
    "# first year\n",
    "ages = range(0, 20)\n",
    "emp_sum = pd.DataFrame(index=ages)\n",
    "emp_count = pd.DataFrame(index=ages)\n",
    "year = years[0]\n",
    "print(year, end=' ')\n",
    "df = pd.read_csv(f'data/synig/{year}.csv', usecols=['ABI', 'EMPLOYEES'])\n",
    "df['AGE'] = 0\n",
    "emp_sum[year] = df.groupby('AGE')['EMPLOYEES'].sum()\n",
    "emp_count[year] = df.groupby('AGE')['EMPLOYEES'].count()\n",
    "del df['EMPLOYEES']\n",
    "df_prev = df\n",
    "\n",
    "for year in years[1:]:\n",
    "    print(year, end=' ')\n",
    "    df = pd.read_csv(f'data/synig/{year}.csv', usecols=['ABI', 'EMPLOYEES'])\n",
    "    df = df.drop_duplicates('ABI')\n",
    "    df = df.merge(df_prev, 'left', 'ABI')\n",
    "    # non-missing values are incremented\n",
    "    df['AGE'] += 1\n",
    "    # missing values are new establishments\n",
    "    df['AGE'] = df['AGE'].fillna(0)\n",
    "    emp_sum[year] = df.groupby('AGE')['EMPLOYEES'].sum()\n",
    "    emp_count[year] = df.groupby('AGE')['EMPLOYEES'].count()\n",
    "    del df['EMPLOYEES']\n",
    "    df_prev = df\n",
    "print()\n",
    "\n",
    "result = emp_sum.sum(1) / emp_count.sum(1)\n",
    "mon.stop()\n",
    "mon.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
