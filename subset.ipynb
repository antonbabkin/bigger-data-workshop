{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting and split-apply-combine\n",
    "\n",
    "Subsetting: only load columns and rows that you need.\n",
    "\n",
    "Split-apply-combine strategy:\n",
    "- **split** your data in smaller subsets\n",
    "- **apply** necessary transformation to subsets one at a time, storing transformation results\n",
    "- **combine** results from subsets to get final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigger than memory\n",
    "\n",
    "Loading the whole example dataset in a DataFrame (20M rows, 15 cols) will take about 1 minute and occupy 4GB+ in memory. More memory will be used if you start running computations. Not feasible in Binder environment where memory is limited to 1-2GB, it will crash and restart your kernel. Multiply by 10 for actual InfoGroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this cell in Binder will restart your kernel\n",
    "df = []\n",
    "for year in range(2001, 2021):\n",
    "    df.append(pd.read_csv(f'data/synig/{year}.csv'))\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use a subset for development and testing\n",
    "\n",
    "If data rows are in random order, reading just the first few rows will give you a representative sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for year in range(2001, 2021):\n",
    "    df.append(pd.read_csv(f'data/synig/{year}.csv', nrows=10_000))\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('SECTOR')['EMPLOYEES'].agg(['size', 'sum', 'mean']).astype(int).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not getting all sectors of the economy here. Clearly, row order is not random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a random sample\n",
    "\n",
    "Let's create a random 1% sample. I will only use subset of years to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for year in range(2001, 2006):\n",
    "    d = pd.read_csv(f'data/synig/{year}.csv')\n",
    "    d = d.sample(frac=0.01)\n",
    "    df.append(d)\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with this simple approach on our dataset: longitudinal histories are broken. It won't help if we could even load all years of data and sample from that. Solution: draw random sample of unique identifiers and then get full histories for those identifiers. This approach will yield a sample that has the same distribution as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abi = []\n",
    "for year in range(2001, 2021):\n",
    "    abi.append(pd.read_csv(f'data/synig/{year}.csv', usecols=['ABI']))\n",
    "abi = pd.concat(abi)\n",
    "abi = abi.drop_duplicates()\n",
    "abi = abi.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will take about 1.5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for year in range(2001, 2021):\n",
    "    d = pd.read_csv(f'data/synig/{year}.csv')\n",
    "    d = d.merge(abi, 'left', 'ABI', indicator=True)\n",
    "    d = d[d['_merge'] == 'both']\n",
    "    del d['_merge']\n",
    "    df.append(d)\n",
    "    print('finished year', year)\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this lightweight sample to get some insights about the whole, for example, compare sector sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('SECTOR')['EMPLOYEES'].agg(['size', 'sum', 'mean']).astype(int).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### persist intermediate data for later use\n",
    "\n",
    "You can save dataframe as CSV, `parquet` (stay tuned) or some other storage format. Or use standard Python `pickle` module. Here I am using `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(df, 'data/rand_1pct.pkl')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart kernel and import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1p = joblib.load('data/rand_1pct.pkl')\n",
    "df1p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example 1: annual aggregates\n",
    "\n",
    "For each year, compute total number of establishments, total and average employment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 1: load all years into single dataframe\n",
    "\n",
    "If we could fit data in memory, we would simply do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1p.groupby('YEAR')['EMPLOYEES'].agg(['size', 'sum', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually even with 1GB memory limit, we are able to load all years of data, because we only need two columns. Let's do this for  comparison with other methods.\n",
    "\n",
    "*If this hits memory limit, try after restarting kernel or reduce number of years.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for year in range(2001, 2021):\n",
    "    print(year, end=' ')\n",
    "    d = pd.read_csv(f'data/synig/{year}.csv', usecols=['YEAR', 'EMPLOYEES'])\n",
    "    df.append(d)\n",
    "print()\n",
    "df = pd.concat(df)\n",
    "result = df.groupby('YEAR')['EMPLOYEES'].agg(['size', 'sum', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.astype(int).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 2: split by year\n",
    "\n",
    "Now let's use split-apply-combine. We load data year by year, compute aggregates and save them, and then combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(index=['size', 'sum', 'mean'])\n",
    "# split by year\n",
    "for year in range(2001, 2021):\n",
    "    print(year, end=' ')\n",
    "    df = pd.read_csv(f'data/synig/{year}.csv', usecols=['EMPLOYEES'])\n",
    "    # apply transformation\n",
    "    res = df.agg(['size', 'sum', 'mean'])\n",
    "    # combine year into final result\n",
    "    result[year] = res\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 3: split by year and chunking\n",
    "\n",
    "Let's pretend that loading even a single year at a time is not feasible. We can use `pd.read_csv(chunksize=)` to only load a small chunk of the whole CSV. This becomes a nested split-apply-combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(index=['size', 'sum', 'mean'])\n",
    "# split by year\n",
    "for year in range(2001, 2021):\n",
    "    print(year, end=': ')\n",
    "    res = pd.DataFrame(index=['size', 'sum'])\n",
    "    # split by chunk\n",
    "    for i, d in enumerate(pd.read_csv(f'data/synig/{year}.csv', usecols=['EMPLOYEES'], chunksize=100_000)):\n",
    "        print(i, end=' ')\n",
    "        # apply transformation\n",
    "        # note: we don't compute mean here, because combining chunk means is not trivial\n",
    "        res[i] = d.agg(['size', 'sum'])\n",
    "        count_notna += d['EMPLOYEES'].notna().sum()\n",
    "    # combine chunk results\n",
    "    res = res.sum(1) \n",
    "    res['mean'] = res['sum'] / count_notna\n",
    "    # combine year into final result\n",
    "    result[year] = res\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measuring\n",
    "\n",
    "We will use `ResourceMonitor` to compare resource usage between three methods.\n",
    "\n",
    "To make sure that no data is hanging in memory between runs, I wrap each method in a function, so dataframes can be garbage collected once function returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel before running\n",
    "import time\n",
    "import pandas as pd\n",
    "from tools import ResourceMonitor\n",
    "\n",
    "years = range(2001, 2021)\n",
    "\n",
    "def method1_load_all():\n",
    "    print('Start method 1')\n",
    "    t0 = time.time()\n",
    "    df = []\n",
    "    for year in years:\n",
    "        print(year, end=' ')\n",
    "        d = pd.read_csv(f'data/synig/{year}.csv', usecols=['YEAR', 'EMPLOYEES'])\n",
    "        df.append(d)\n",
    "    print()\n",
    "    df = pd.concat(df)\n",
    "    result = df.groupby('YEAR')['EMPLOYEES'].agg(['size', 'sum', 'mean'])\n",
    "    dt = time.time() - t0\n",
    "    print(f'Finish method 1 in {dt:.1f} seconds\\n')\n",
    "    return result.T\n",
    "\n",
    "def method2_split_by_year():\n",
    "    print('Start method 2')\n",
    "    t0 = time.time()\n",
    "    result = pd.DataFrame(index=['size', 'sum', 'mean'])\n",
    "    for year in years:\n",
    "        print(year, end=' ')\n",
    "        df = pd.read_csv(f'data/synig/{year}.csv', usecols=['EMPLOYEES'])\n",
    "        res = df.agg(['size', 'sum', 'mean'])\n",
    "        result[year] = res\n",
    "    print()\n",
    "    dt = time.time() - t0\n",
    "    print(f'Finish method 2 in {dt:.1f} seconds\\n')\n",
    "    return result\n",
    "\n",
    "def method3_split_by_year_chunk():\n",
    "    print('Start method 3')\n",
    "    t0 = time.time()\n",
    "    result = pd.DataFrame(index=['size', 'sum', 'mean'])\n",
    "    for year in years:\n",
    "        print(year, end=': ')\n",
    "        res = pd.DataFrame(index=['size', 'sum'])\n",
    "        count_notna = 0\n",
    "        for i, d in enumerate(pd.read_csv(f'data/synig/{year}.csv', usecols=['EMPLOYEES'], chunksize=100_000)):\n",
    "            print(i, end=' ')\n",
    "            res[i] = d.agg(['size', 'sum'])\n",
    "            count_notna += d['EMPLOYEES'].notna().sum()\n",
    "        res = res.sum(1) \n",
    "        res['mean'] = res['sum'] / count_notna\n",
    "        result[year] = res\n",
    "        print()\n",
    "    dt = time.time() - t0\n",
    "    print(f'Finish method 3 in {dt:.1f} seconds\\n')\n",
    "    return result\n",
    "\n",
    "\n",
    "mon = ResourceMonitor()\n",
    "mon.start()\n",
    "time.sleep(1) # give monitor time to start\n",
    "mon.tag('method 1 V')\n",
    "r1 = method1_load_all()\n",
    "time.sleep(1)\n",
    "mon.tag('method 2 V')\n",
    "r2 = method2_split_by_year()\n",
    "time.sleep(1)\n",
    "mon.tag('method 3 V')\n",
    "r3 = method3_split_by_year_chunk()\n",
    "time.sleep(1)\n",
    "mon.stop()\n",
    "\n",
    "# make sure that all methods yield same results\n",
    "assert (r1 == r2).all().all()\n",
    "assert (r1 == r3).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take closer look at data stored by monitor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                 | Method 1 | Method 2 | Method 3 |\n",
    "|-----------------|:--------:|:--------:|:--------:|\n",
    "| Code complexity |    low   |  medium  |   high   |\n",
    "| Running time    |   fast   |  medium  |   slow   |\n",
    "| Memory usage    |   high   |  medium  |    low   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example 2: size vs age"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
