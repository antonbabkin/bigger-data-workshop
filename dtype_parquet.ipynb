{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data type optimization and parquet storage\n",
    "\n",
    "- Data types automatically chosen by `pandas.read_csv()` may not always be optimal.\n",
    "  - leading zeros in ZIP codes\n",
    "  - 8 bytes per value where 1 byte would suffice\n",
    "- String columns use up a lot of memory, convert them to categoricals when number of unique values is not too big relative to number of observations.\n",
    "- Parquet storage format preserves dtype information and enables partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data types\n",
    "\n",
    "Pandas columns are internally stored as numpy arrays, and so [NumPy data types](https://numpy.org/doc/stable/user/basics.types.html) are used.\n",
    "\n",
    "**Boolean**\n",
    "\n",
    "`np.bool_` takes 1 byte per item, but can not hold missing values. Logical operations on columns return series of this dtype, unless some of the element-wise tests results in NA value, in which case result is of `object` dtype.\n",
    "\n",
    "\n",
    "**Integer**\n",
    "\n",
    "Limits and other details can be looked up with `numpy.iinfo()`.\n",
    "\n",
    "Storing value outside of limits creates overflow.\n",
    "\n",
    "|  dtype | size (bytes) |             min            |             max            |\n",
    "|:------:|:------------:|:--------------------------:|:--------------------------:|\n",
    "| uint8  |       1      | 0                          | 255                        |\n",
    "| uint16 |       2      | 0                          | 65,535                     |\n",
    "| uint32 |       4      | 0                          | 4,294,967,295              |\n",
    "| uint64 |       8      | 0                          | 18,446,744,073,709,551,615 |\n",
    "| int8   |       1      | -128                       | 127                        |\n",
    "| int16  |       2      | -32,768                    | 32,767                     |\n",
    "| int32  |       4      | -2,147,483,648             | 2,147,483,647              |\n",
    "| int64  |       8      | -9,223,372,036,854,775,808 | 9,223,372,036,854,775,807  |\n",
    "\n",
    "Integer dtypes provide wide range of options, but the biggest constraint is that in standard pandas these dtypes do not allow for missing values in them.\n",
    "\n",
    "**Floating point**\n",
    "\n",
    "Wikipedia: [float16](https://en.wikipedia.org/wiki/Half-precision_floating-point_format), [float32](https://en.wikipedia.org/wiki/Single-precision_floating-point_format), [float64](https://en.wikipedia.org/wiki/Double-precision_floating-point_format).\n",
    "\n",
    "Limits and other details can be looked up with `numpy.finfo()`.\n",
    "\n",
    "Spacing between a number and it's adjacent neighbor (`numpy.spacing()`) increases with number absolute magnitude. Therefore care should be taken when storing large integers as floats.\n",
    "\n",
    "\n",
    "|  dtype  | size (bytes) |           max           |       max exact integer       |\n",
    "|:-------:|:------------:|:-----------------------:|:-----------------------------:|\n",
    "| float16 |       2      |       6.55040e+04       |         $2^{11}$ = 2,048         |\n",
    "| float32 |       4      |      3.4028235e+38      |       $2^{24}$ = 16,777,216      |\n",
    "| float64 |       8      | 1.7976931348623157e+308 | $2^{53}$ = 9,007,199,254,740,992 |\n",
    "\n",
    "Even though `float16` might have good use cases (notably booleans with missing data), it looks like it is not always fully supported.\n",
    "\n",
    "*Floats are used by pandas to store integers with missing values.*\n",
    "\n",
    "**Strings**\n",
    "\n",
    "Although there are fixed length Unicode string dtype in NumPy (e.g. `np.dtype('U3')`), pandas uses `np.object_`. This is an array of pointers (item size of 32 or 64 bits, depending on platform architecture) to memory locations where actual strings are stored.\n",
    "\n",
    "**Date and time***\n",
    "\n",
    "You can turn string dates and times into special [dtype](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html) that supports multiple special functions.\n",
    "\n",
    "### Categoricals\n",
    "\n",
    "[Categorical variables](https://pandas.pydata.org/docs/user_guide/categorical.html) are a great memory when using string data.\n",
    "\n",
    "### Experimental nullable dtypes\n",
    "\n",
    "Recent versions of pandas have new data types that support new form of [missing values](https://pandas.pydata.org/docs/user_guide/integer_na.html). With them, we won't need to use floats to store integers. The feature is still experimental though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet\n",
    "\n",
    "- Binary data: data type is preserved\n",
    "- Columnar storage: efficient reading of subset of columns and dtype-specific compression\n",
    "- Partitioning: only read chunks that satisfy a given condition\n",
    "  - Every partition adds metadata overhead. With too many partitions, this can incur significant performance cost. For example, if SynIG is partitioned by YEAR, STATE and SECTOR (about 17,000 partitions), it becomes much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fastparquet\n",
    "\n",
    "from tools import ResourceMonitor, state_00_aa\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert SynIG from CSV to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = ['11', '21', '22', '23', '31', '42', '44', '48', '51', '52',\n",
    "           '53', '54', '55', '56', '61', '62', '71', '72', '81', '92', '99']\n",
    "states = list(state_00_aa.values())\n",
    "\n",
    "def convert_synig_dtypes(df):\n",
    "    if 'STATE' in df:\n",
    "        df['STATE'] = pd.Categorical(df['STATE'], states)\n",
    "    if 'SECTOR' in df:\n",
    "        df['SECTOR'] = pd.Categorical(df['SECTOR'], sectors)\n",
    "    if 'EMPLOYEES_CODE' in df:\n",
    "        df['EMPLOYEES_CODE'] = pd.Categorical(df['EMPLOYEES_CODE'], list('ABCDEFGHIJK'), ordered=True)\n",
    "    for c in ['EMPLOYEES', 'LONGITUDE', 'LATITUDE']:\n",
    "        if c in df:\n",
    "            df[c] = df[c].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "years = range(2001, 2021)\n",
    "years = years[:5]\n",
    "paths = []\n",
    "for year in years:\n",
    "    print(year, end=' ')\n",
    "    df = pd.read_csv(f'data/synig/{year}.csv', dtype=str)\n",
    "    del df['YEAR']\n",
    "    convert_synig_dtypes(df)\n",
    "    path = f'data/synig.pq/YEAR={year}'\n",
    "    fastparquet.write(path, df, file_scheme='hive', write_index=False, partition_on=['STATE'])\n",
    "    paths.append(path)\n",
    "pf = fastparquet.writer.merge(paths)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read one year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = ResourceMonitor(interval=0.3)\n",
    "def read_csv():\n",
    "    mon.tag('read csv')\n",
    "    df = pd.read_csv('data/synig/2001.csv', dtype=str)\n",
    "    mon.tag('convert')\n",
    "    convert_synig_dtypes(df)\n",
    "def read_pq():\n",
    "    mon.tag('read pq')\n",
    "    df = pd.read_parquet('data/synig.pq', filters=[('YEAR', '==', 2001)])\n",
    "\n",
    "mon.start()\n",
    "sleep(1)\n",
    "read_csv()\n",
    "sleep(1)\n",
    "read_pq()\n",
    "sleep(1)\n",
    "mon.stop()\n",
    "mon.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read one state\n",
    "\n",
    "Subset of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = ResourceMonitor(interval=0.3)\n",
    "years = range(2001, 2021)\n",
    "years = years[:5]\n",
    "state = 'WI'\n",
    "cols = ['YEAR', 'STATE', 'SECTOR', 'EMPLOYEES', 'NAICS']\n",
    "\n",
    "def read_csv():\n",
    "    mon.tag('read csv')\n",
    "    df = []\n",
    "    for year in years:\n",
    "        print(year, end=' ')\n",
    "        d = pd.read_csv(f'data/synig/{year}.csv', dtype=str, usecols=cols)\n",
    "        convert_synig_dtypes(d)\n",
    "        d = d[d['STATE'] == state]\n",
    "        df.append(d)\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    print()\n",
    "    print(df.shape)\n",
    "    sleep(1)\n",
    "    \n",
    "def read_pq():\n",
    "    mon.tag('read pq')\n",
    "    df = pd.read_parquet('data/synig.pq', columns=cols, \n",
    "                         filters=[('YEAR', 'in', years), ('STATE', '==', 'WI')])\n",
    "    print(df.shape)\n",
    "    sleep(1)\n",
    "\n",
    "mon.start()\n",
    "sleep(1)\n",
    "read_csv()\n",
    "sleep(1)\n",
    "read_pq()\n",
    "mon.stop()\n",
    "mon.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
