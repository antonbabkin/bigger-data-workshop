{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data type optimization and parquet storage\n",
    "\n",
    "- Data types automatically chosen by `pandas.read_csv()` may not always be optimal.\n",
    "  - leading zeros in ZIP codes\n",
    "  - 8 bytes per value where 1 byte would suffice\n",
    "- String columns use up a lot of memory, convert them to categoricals when number of unique values is not too big relative to number of observations.\n",
    "- Parquet storage format preserves dtype information and enables partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data types\n",
    "\n",
    "Pandas columns are internally stored as numpy arrays, and so [NumPy data types](https://numpy.org/doc/stable/user/basics.types.html) are used.\n",
    "\n",
    "**Boolean**\n",
    "\n",
    "`np.bool_` takes 1 byte per item, but can not hold missing values. Logical operations on columns return series of this dtype, unless some of the element-wise tests results in NA value, in which case result is of `object` dtype.\n",
    "\n",
    "\n",
    "**Integer**\n",
    "\n",
    "Limits and other details can be looked up with `numpy.iinfo()`.\n",
    "\n",
    "Storing value outside of limits creates overflow.\n",
    "\n",
    "|  dtype | size (bytes) |             min            |             max            |\n",
    "|:------:|:------------:|:--------------------------:|:--------------------------:|\n",
    "| uint8  |       1      | 0                          | 255                        |\n",
    "| uint16 |       2      | 0                          | 65,535                     |\n",
    "| uint32 |       4      | 0                          | 4,294,967,295              |\n",
    "| uint64 |       8      | 0                          | 18,446,744,073,709,551,615 |\n",
    "| int8   |       1      | -128                       | 127                        |\n",
    "| int16  |       2      | -32,768                    | 32,767                     |\n",
    "| int32  |       4      | -2,147,483,648             | 2,147,483,647              |\n",
    "| int64  |       8      | -9,223,372,036,854,775,808 | 9,223,372,036,854,775,807  |\n",
    "\n",
    "Integer dtypes provide wide range of options, but the biggest constraint is that in standard pandas these dtypes do not allow for missing values in them.\n",
    "\n",
    "**Floating point**\n",
    "\n",
    "Wikipedia: [float16](https://en.wikipedia.org/wiki/Half-precision_floating-point_format), [float32](https://en.wikipedia.org/wiki/Single-precision_floating-point_format), [float64](https://en.wikipedia.org/wiki/Double-precision_floating-point_format).\n",
    "\n",
    "Limits and other details can be looked up with `numpy.finfo()`.\n",
    "\n",
    "Spacing between a number and it's adjacent neighbor (`numpy.spacing()`) increases with number absolute magnitude. Therefore care should be taken when storing large integers as floats.\n",
    "\n",
    "\n",
    "|  dtype  | size (bytes) |           max           | precision (significant decimal digits) |         max exact integer        |\n",
    "|:-------:|:------------:|:-----------------------:|:--------------------------------------:|:--------------------------------:|\n",
    "| float16 |       2      |       6.55040e+04       |                 3 to 4                 |         $2^{11}$ = 2,048         |\n",
    "| float32 |       4      |      3.4028235e+38      |                 6 to 9                 |       $2^{24}$ = 16,777,216      |\n",
    "| float64 |       8      | 1.7976931348623157e+308 |                15 to 17                | $2^{53}$ = 9,007,199,254,740,992 |\n",
    "\n",
    "Even though `float16` might have good use cases (notably booleans with missing data), it is not always fully supported.\n",
    "\n",
    "*Floats are used by pandas to store integers with missing values.*\n",
    "\n",
    "**Date and time**\n",
    "\n",
    "Turn string dates, times and time intervals into 64-bit dtypes `np.datetime64` and `np.timedelta64` that support wide range of [specialized functions](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html).\n",
    "\n",
    "**Strings**\n",
    "\n",
    "Although there are fixed length Unicode string dtype in NumPy (e.g. `np.dtype('U3')`), pandas uses `np.object_`. This is an array of pointers (item size of 32 or 64 bits, depending on platform architecture) to memory locations where actual strings are stored.\n",
    "\n",
    "\n",
    "\n",
    "### Categoricals\n",
    "\n",
    "[Categorical variables](https://pandas.pydata.org/docs/user_guide/categorical.html) are a great way to reduce memory usage when working with string data. Instead of storing one string per observation (and a pointer to it), column will only contain category codes (which in most use cases fit into `np.int8`) and an overhead with codes-to-labels mapping. Operations on categorical columns (select, groupby) will also be faster than on string columns. If you perform an operation on two categorical columns (compare, merge), make sure that their categories are the same in order to realize potential performance gains.\n",
    "\n",
    "### Experimental nullable dtypes\n",
    "\n",
    "Recent versions of pandas have new data types that support new form of [missing values](https://pandas.pydata.org/docs/user_guide/integer_na.html). With them, we won't need to use floats to store integers. The feature is still experimental though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Load a subset of SynIG columns and compare memory usage under different conversion regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = ['STATE', 'SECTOR', 'NAICS', 'EMPLOYEES', 'EMPLOYEES_CODE', 'COUNTY_CODE', 'LONGITUDE', 'LATITUDE']\n",
    "\n",
    "# default conversion\n",
    "df0 = pd.read_csv('data/synig/2001.csv', usecols=cols, nrows=100_000)\n",
    "\n",
    "# no conversion\n",
    "df1 = pd.read_csv('data/synig/2001.csv', usecols=cols, nrows=100_000, dtype='str')\n",
    "\n",
    "# custom conversion\n",
    "from tools import state_00_aa\n",
    "sectors = ['11', '21', '22', '23', '31', '42', '44', '48', '51', '52',\n",
    "           '53', '54', '55', '56', '61', '62', '71', '72', '81', '92', '99']\n",
    "states = list(state_00_aa.values())\n",
    "dt = {\n",
    "    'STATE': pd.CategoricalDtype(sectors),\n",
    "    'SECTOR': pd.CategoricalDtype(states),\n",
    "    'NAICS': 'str',\n",
    "    'EMPLOYEES': 'float32',\n",
    "    'EMPLOYEES_CODE': pd.CategoricalDtype(list('ABCDEFGHIJK'), ordered=True),\n",
    "    'COUNTY_CODE': 'str',\n",
    "    'LONGITUDE': 'float64',\n",
    "    'LATITUDE': 'float64'\n",
    "}\n",
    "df2 = pd.read_csv('data/synig/2001.csv', usecols=cols, nrows=100_000, dtype=dt)\n",
    "\n",
    "def dt_mem(df):\n",
    "    mem = (df.memory_usage(index=False, deep=True) / 1e6).round(1)\n",
    "    return pd.concat([df.dtypes, mem], 1).rename(columns={0: 'dtype', 1: 'mem, MB'})\n",
    "\n",
    "pd.concat({'default': dt_mem(df0), \n",
    "           'no conversion': dt_mem(df1),\n",
    "           'custom': dt_mem(df2)}, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet\n",
    "\n",
    "- Binary format: data type is preserved\n",
    "- Columnar storage: efficient reading of subset of columns and dtype-specific compression\n",
    "- Partitioning: only read chunks that satisfy a given condition\n",
    "  - Every partition adds metadata overhead. With too many partitions, this can incur significant performance cost. For example, if SynIG is partitioned by YEAR, STATE and SECTOR (about 17,000 partitions), it becomes much slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tools import convert_synig_dtypes\n",
    "\n",
    "df = pd.read_csv('data/synig/2001.csv', dtype='str')\n",
    "convert_synig_dtypes(df)\n",
    "df.to_parquet('data/synig_2001.pq', index=False, partition_cols=['STATE'])\n",
    "\n",
    "df0 = df\n",
    "df1 = pd.read_parquet('data/synig_2001.pq')\n",
    "\n",
    "pd.concat({'before storage': dt_mem(df0), \n",
    "           'after loading': dt_mem(df1)}, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now efficienly load subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/synig_2001.pq', columns=['SECTOR', 'EMPLOYEES'],\n",
    "                     filters=[('STATE', 'in', ['WI', 'CT'])])\n",
    "(df.groupby(['SECTOR', 'STATE'])['EMPLOYEES'].sum()\n",
    " .dropna().unstack().fillna(0).astype(int).style.format('{:,d}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert SynIG from CSV to parquet\n",
    "\n",
    "We want to store entire SynIG dataset (all years and columns) to parquet, partitioned by YEAR and STATE. We can not save it all in one go, because doing so would require loading it all in memory at once. So we use split-apply-combine strategy to process year by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# remove old 'data/synig.pq' and restart kernel before running\n",
    "\n",
    "import pandas as pd\n",
    "import fastparquet\n",
    "from tools import convert_synig_dtypes\n",
    "\n",
    "years = range(2001, 2021)\n",
    "# reduce number of years for faster demonstration\n",
    "years = years[:3]\n",
    "paths = []\n",
    "for year in years:\n",
    "    print(year, end=' ')\n",
    "    df = pd.read_csv(f'data/synig/{year}.csv', dtype=str)\n",
    "    del df['YEAR']\n",
    "    convert_synig_dtypes(df)\n",
    "    path = f'data/synig.pq/YEAR={year}'\n",
    "    fastparquet.write(path, df, file_scheme='hive', write_index=False, partition_on=['STATE'])\n",
    "    paths.append(path)\n",
    "pf = fastparquet.writer.merge(paths)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tools import convert_synig_dtypes, ResourceMonitor\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read one year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = ResourceMonitor(interval=0.3)\n",
    "def read_csv():\n",
    "    mon.tag('read csv')\n",
    "    df = pd.read_csv('data/synig/2001.csv', dtype=str)\n",
    "    mon.tag('convert')\n",
    "    convert_synig_dtypes(df)\n",
    "    print(df.shape)\n",
    "def read_pq():\n",
    "    mon.tag('read pq')\n",
    "    df = pd.read_parquet('data/synig.pq', filters=[('YEAR', '==', 2001)])\n",
    "    print(df.shape)\n",
    "\n",
    "mon.start()\n",
    "sleep(1)\n",
    "read_csv()\n",
    "sleep(1)\n",
    "read_pq()\n",
    "sleep(1)\n",
    "mon.stop()\n",
    "mon.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read one state\n",
    "\n",
    "Subset of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = ResourceMonitor(interval=0.3)\n",
    "years = range(2001, 2021)\n",
    "# reduce number of years for faster demonstration\n",
    "years = years[:3]\n",
    "state = 'WI'\n",
    "cols = ['YEAR', 'STATE', 'SECTOR', 'EMPLOYEES', 'NAICS', 'LONGITUDE', 'LATITUDE']\n",
    "\n",
    "def read_csv():\n",
    "    mon.tag('read csv')\n",
    "    df = []\n",
    "    for year in years:\n",
    "        print(year, end=' ')\n",
    "        d = pd.read_csv(f'data/synig/{year}.csv', dtype=str, usecols=cols)\n",
    "        convert_synig_dtypes(d)\n",
    "        d = d[d['STATE'] == state]\n",
    "        df.append(d)\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    print()\n",
    "    print(df.shape)\n",
    "    sleep(1)\n",
    "    \n",
    "def read_pq():\n",
    "    mon.tag('read pq')\n",
    "    df = pd.read_parquet('data/synig.pq', columns=cols, \n",
    "                         filters=[('YEAR', 'in', years), ('STATE', '==', state)])\n",
    "    print(df.shape)\n",
    "    sleep(1)\n",
    "\n",
    "mon.start()\n",
    "sleep(1)\n",
    "read_csv()\n",
    "sleep(1)\n",
    "read_pq()\n",
    "mon.stop()\n",
    "mon.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
