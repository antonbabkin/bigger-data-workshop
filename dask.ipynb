{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "Dask is a flexible library for parallel computing in Python.\n",
    "\n",
    "Dask is composed of two parts:\n",
    "1. Dynamic task scheduling optimized for computation.\n",
    "2. “Big Data” collections like parallel arrays, dataframes, and lists.\n",
    "\n",
    "![Dask overview](https://docs.dask.org/en/latest/_images/dask-overview.svg)  \n",
    "Image source: Dask documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task graphs\n",
    "\n",
    "Suppose we want to perform a simple computation: increment numbers 1 and 2 and add up results.\n",
    "$$(1+1) + (2+1)$$\n",
    "\n",
    "To make execution times more visible, assume that every operation takes a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential execution of the 3 operations will take 3 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that first two operations (`inc(1)` and `inc(2)`) are independent and can be executed in parallel, while the last operation depends on the results of the first two.\n",
    "\n",
    "If we wrap our funcitons in `dask.delayed` and then call them, instead of computation we will get a `Delayed` object, which is **task graph**. Execution is instant, because no actual computation is performed at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "\n",
    "delayed_inc = delayed(inc)\n",
    "delayed_add = delayed(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = delayed_inc(1)\n",
    "y = delayed_inc(2)\n",
    "z = delayed_add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call `Delayed.compute()` method to execute the task graph and obtain result. How long do you think it would take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and unzip census tracts shapefiles\n",
    "\n",
    "`dask.delayed` can be used as *decorator*. This is just a more compact way to write wrapping code.\n",
    "\n",
    "```python\n",
    "@delayed\n",
    "def fun():\n",
    "   return 1\n",
    "```\n",
    "is equivalent to\n",
    "```python\n",
    "def fun():\n",
    "   return 1\n",
    "fun = delayed(fun)\n",
    "```\n",
    "\n",
    "Here we repeat the process of downloading and unzipping census state tracts, but this time we delegate parallelization and scheduling to dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import download_file, unzip, tracts_state_00_aa\n",
    "from dask import compute\n",
    "\n",
    "@delayed\n",
    "def download_state_tracts(state_code):\n",
    "    url = f'https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_{state_code}_tract_500k.zip'\n",
    "    f = download_file(url, f'data/tracts/{state_code}', overwrite=True, verbose=False)\n",
    "    return state_code, f\n",
    "\n",
    "@delayed\n",
    "def unzip_state_tracts(state_code_and_filepath):\n",
    "    state_code, filepath = state_code_and_filepath\n",
    "    unzip(filepath, f'data/tracts/{state_code}', overwrite=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plan = []\n",
    "for sc in tracts_state_00_aa.keys():\n",
    "    code_and_path = download_state_tracts(sc)\n",
    "    z = unzip_state_tracts(code_and_path)\n",
    "    plan.append(z)\n",
    "_ = compute(plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask arrays\n",
    "\n",
    "Have the same interface as NumPy arrays, but are internally represented as multiple chunks. Computations on chunks are turned into a task graph and performed in parallel where possible.\n",
    "\n",
    "NumPy is already using very performant low level code for linear algebra, but all data needs to be in memory. Dask relaxes this constraint.\n",
    "\n",
    "![Dask array](https://docs.dask.org/en/latest/_images/dask-array-black-text.svg)  \n",
    "Image source: Dask documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to estimate a linear regression model using ordinary least squares ([wikipedia](https://en.wikipedia.org/wiki/Ordinary_least_squares)).\n",
    "\n",
    "The model is given by the following eqation.\n",
    "$$y = \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + e$$\n",
    "\n",
    "In matrix notation: \n",
    "$$y = X \\beta + e$$\n",
    "where $y$ and $e$ are vectors of length $n$, $X$ is $n$-by-$k$ matrix, and $\\beta$ is vector of length $k$.\n",
    "\n",
    "OLS estimates $\\hat{\\beta}$ can be calculated as\n",
    "$$\\hat{\\beta} = (X'X)^{-1}X'y$$\n",
    "\n",
    "First let's generate some random data with $\\beta = [1, 2, ..., k]$ and store it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def generate_data(n, k):\n",
    "    b = np.arange(1, k+1)\n",
    "    x = np.random.rand(n, k)\n",
    "    y = x.dot(b) + np.random.rand(n)\n",
    "\n",
    "    Path('data/arr').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # single files for numpy\n",
    "    np.save('data/arr/x.npy', x)\n",
    "    np.save('data/arr/y.npy', y)\n",
    "\n",
    "    # directories for dask arrays\n",
    "    da.to_npy_stack('data/arr/x/', da.from_array(x))\n",
    "    da.to_npy_stack('data/arr/y/', da.from_array(y))\n",
    "\n",
    "generate_data(1_000_000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate OLS with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ols_numpy():\n",
    "    x = np.load('data/arr/x.npy')\n",
    "    y = np.load('data/arr/y.npy')\n",
    "    xpxi = np.linalg.inv(x.T.dot(x))\n",
    "    xpy = x.T.dot(y)\n",
    "    b_hat = xpxi.dot(xpy)\n",
    "    return b_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bh = ols_numpy()\n",
    "bh[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delayed version is created if we replace `numpy.array` with `dask.array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "def ols_dask():\n",
    "    x = da.from_npy_stack('data/arr/x')\n",
    "    y = da.from_npy_stack('data/arr/y')\n",
    "    xpxi = da.linalg.inv(x.T.dot(x))\n",
    "    xpy = x.T.dot(y)\n",
    "    b_hat = xpxi.dot(xpy)\n",
    "    return b_hat\n",
    "\n",
    "plan = ols_dask()\n",
    "plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bh = plan.compute()\n",
    "bh[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the task graph is rather complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan.visualize('tasks.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel and redefine functions to get cleaner results\n",
    "from time import sleep\n",
    "from tools import ResourceMonitor\n",
    "\n",
    "mon = ResourceMonitor(interval=0.1)\n",
    "mon.start()\n",
    "sleep(1)\n",
    "mon.tag('numpy V')\n",
    "b_numpy = ols_numpy()\n",
    "mon.tag('numpy ^')\n",
    "sleep(1)\n",
    "mon.tag('dask V')\n",
    "b_dask = ols_dask().compute()\n",
    "mon.tag('dask ^')\n",
    "sleep(1)\n",
    "mon.stop()\n",
    "\n",
    "assert np.allclose(b_numpy, b_dask)\n",
    "\n",
    "mon.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask used more CPU and less memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask dataframes\n",
    "\n",
    "Similarly to arrays, dask dataframes are chunked for parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example revisited: employment by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from tools import ResourceMonitor\n",
    "from time import sleep\n",
    "\n",
    "df = dd.read_csv('data/synig/*.csv', usecols=['YEAR', 'EMPLOYEES'])\n",
    "res = df.groupby('YEAR')['EMPLOYEES'].agg(['size', 'sum', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.visualize('tasks.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = ResourceMonitor(interval=0.3)\n",
    "mon.start()\n",
    "sleep(0.5)\n",
    "r = res.compute()\n",
    "sleep(0.5)\n",
    "mon.stop()\n",
    "mon.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example revisited: size vs age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv('data/synig/*.csv', usecols=['YEAR', 'ABI', 'EMPLOYEES'])\n",
    "fy = df.groupby('ABI')['YEAR'].min().to_frame('FIRST_YEAR')\n",
    "df = df.merge(fy, 'left', 'ABI')\n",
    "df['AGE'] = df['YEAR'] - df['FIRST_YEAR']\n",
    "res = df.groupby('AGE')['EMPLOYEES'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.visualize('tasks.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = ResourceMonitor()\n",
    "mon.start()\n",
    "sleep(0.5)\n",
    "r = res.compute()\n",
    "sleep(0.5)\n",
    "mon.stop()\n",
    "mon.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory trade-offs\n",
    "\n",
    "|                 | in memory | split-apply-combine | dask |\n",
    "|-----------------|:--------:|:--------:|:--------:|\n",
    "| Code complexity |    low   |  medium/high  |   low-high   |\n",
    "| Running time    |   fast   |  medium  |   medium   |\n",
    "| Memory usage    |   high   |  varies  |    varies   |\n",
    "\n",
    "\n",
    "Dask is a powerful tool that is easy to start with, because it mimics interface of `numpy` and `pandas`. But here are some things to keep in mind.\n",
    "- You will need to learn how Dask works to approach more difficult computations.\n",
    "- Some operations in Dask are slow. Some are not available (eg. `df.sort_values()` remains unimplemented for [5 years](https://github.com/dask/dask/issues/958)). Some work slightly differently than in pandas (`df.drop_duplicates(keep='first')`).\n",
    "- Debugging of distributed algorithms is more difficult.\n",
    "- Chunk size and number of workers need to be tuned to your hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn more\n",
    "\n",
    "[Dask homepage](https://dask.org/)  \n",
    "[Docs](https://docs.dask.org/en/latest/)  \n",
    "An excellent interactive tutorial from Dask creators:\n",
    "[How to learn Dask in 2021](https://coiled.io/blog/how-to-learn-dask-in-2021)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
